{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def showInNetron(model_filename):\n",
    "    netron.start(model_filename, address=(\"0.0.0.0\",8081))\n",
    "    return IFrame(src=\"http://0.0.0.0:8081/\",width=\"100%\",height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.core.datatype import DataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "from finn.core.datatype import DataType\n",
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveFlattenPastAffine\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import os\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "\n",
    "\n",
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile 1D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn-base/src/finn/transformation/infer_data_layouts.py:114: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "model_file=\"./multi_model_test/save_model/mnist_1D_3136_s.onnx\" # modify if model is changed\n",
    "\n",
    "model=ModelWrapper(model_file)\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType.UINT8)\n",
    "model.save(\"./multi_model_test/test_model/mnist_1D_3136_s.onnx\") # modify if model is changed\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"./multi_model_test/test_temp/mnist_1D.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8 \n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"./multi_model_test/test_temp/mnist_1D.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"./multi_model_test/test_temp/mnist_1D.onnx\")\n",
    "\n",
    "\n",
    "# HLS\n",
    "\n",
    "output_dir = \"./multi_model_test/test_temp/mnist_1D/3136_s\" # modify if model is changed\n",
    "\n",
    "if not os.path.exists(output_dir + \"/intermediate_models\"):\n",
    "    os.makedirs(output_dir + \"/intermediate_models\")\n",
    "\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"const\"\n",
    "\n",
    "model = ModelWrapper(\"./multi_model_test/test_temp/mnist_1D.onnx\")\n",
    "\n",
    "\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "\n",
    "\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "model = model.transform(to_hls.InferPool_Batch())\n",
    "\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "\n",
    "\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save(\"./multi_model_test/test_temp/mnist_1D.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(output_dir + \"/intermediate_models/dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(output_dir + \"/intermediate_models/dataflow_model.onnx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if all layers are converted to HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './multi_model_test/test_temp/mnist_1D.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9320aa9a60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"./multi_model_test/test_temp/mnist_1D.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate stitched IP Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from ./multi_model_test/test_temp/mnist_1D.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_wenlong\n",
      "Final outputs will be generated in ./multi_model_test/test_temp/mnist_1D/3136_s\n",
      "Build log is at ./multi_model_test/test_temp/mnist_1D/3136_s/build_dataflow.log\n",
      "Running step: step_create_dataflow_partition [1/8]\n",
      "Running step: step_target_fps_parallelization [2/8]\n",
      "Running step: step_apply_folding_config [3/8]\n",
      "Running step: step_generate_estimate_reports [4/8]\n",
      "Running step: step_hls_codegen [5/8]\n",
      "Running step: step_hls_ipgen [6/8]\n",
      "Running step: step_set_fifo_depths [7/8]\n",
      "Running step: step_create_stitched_ip [8/8]\n",
      "Completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "\n",
    "model_file = \"./multi_model_test/test_temp/mnist_1D.onnx\"\n",
    "\n",
    "rtlsim_output_dir = output_dir\n",
    "\n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    synth_clk_period_ns = 20.0, #50 MHz             # set target frequency\n",
    "    fpga_part           = \"xc7a100tfgg484-1\",       # set target FPGA board\n",
    "    #fpga_part           = \"xc7a15tfgg484-1\", \n",
    "    #fpga_part           = \"xc7a200tfbg484-1\",\n",
    "    \n",
    "    default_mem_mode = build_cfg.ComputeEngineMemMode.CONST,\n",
    "    large_fifo_mem_style = build_cfg.LargeFIFOMemStyle.LUTRAM, # large FIFO set LUTRAM\n",
    "    #large_fifo_mem_style = build_cfg.LargeFIFOMemStyle.AUTO,\n",
    "    \n",
    "    #auto_fifo_depths = False,\n",
    "    \n",
    "    folding_config_file = \"./multi_model_test/config/Least_pe_malexnet_1D_without_BRAM.json\",# choose a configuration according to requirements\n",
    "    #folding_config_file = \"./multi_model_test/config/Least_pe_malexnet_1D_auto.json\",\n",
    "    \n",
    "    steps = [\"step_create_dataflow_partition\",\n",
    "             \"step_target_fps_parallelization\",\n",
    "             \"step_apply_folding_config\",\n",
    "             \"step_generate_estimate_reports\",\n",
    "             \"step_hls_codegen\",\n",
    "             \"step_hls_ipgen\",\n",
    "             \"step_set_fifo_depths\",\n",
    "            \"step_create_stitched_ip\",],\n",
    "             \n",
    "   \n",
    "\n",
    "    \n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "    ]\n",
    ")\n",
    "\n",
    "build.build_dataflow_cfg(model_file, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compile 2D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn-base/src/finn/transformation/infer_data_layouts.py:114: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "model_file=\"./multi_model_test/save_model/model_2D_28_28_s.onnx\"  # modify if model is changed\n",
    "\n",
    "model=ModelWrapper(model_file)\n",
    "model.set_tensor_datatype(model.graph.input[0].name, DataType.UINT8)\n",
    "model.save(\"./multi_model_test/test_model/model_2D_28_28_s.onnx\")  # modify if model is changed\n",
    "\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = \"./multi_model_test/test_temp/model_2D.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType.UINT8)\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "chkpt_name = \"./multi_model_test/test_temp/model_2D.onnx\"\n",
    "# tidy-up again\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(GiveUniqueNodeNames())\n",
    "model = model.transform(GiveReadableTensorNames())\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model.save(chkpt_name)\n",
    "\n",
    "\n",
    "model = model.transform(MoveScalarLinearPastInvariants())\n",
    "model = model.transform(Streamline())\n",
    "model = model.transform(LowerConvsToMatMul())\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(MakeMaxPoolNHWC())\n",
    "model = model.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model = model.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model = model.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model = model.transform(InferDataLayouts())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "model.save(\"./multi_model_test/test_temp/model_2D.onnx\")\n",
    "\n",
    "\n",
    "# HLS\n",
    "\n",
    "output_dir = \"./multi_model_test/test_temp/mnist_2D/28\"\n",
    "\n",
    "if not os.path.exists(output_dir + \"/intermediate_models\"):\n",
    "    os.makedirs(output_dir + \"/intermediate_models\")\n",
    "\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"const\"\n",
    "\n",
    "model = ModelWrapper(\"./multi_model_test/test_temp/model_2D.onnx\")  # modify if model is changed\n",
    "\n",
    "#'''\n",
    "model = model.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model = model.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "\n",
    "\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "model = model.transform(to_hls.InferPool_Batch())\n",
    "\n",
    "model = model.transform(to_hls.InferThresholdingLayer())\n",
    "model = model.transform(to_hls.InferConvInpGen())\n",
    "model = model.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model = model.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model = model.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model = model.transform(InferDataLayouts())\n",
    "\n",
    "\n",
    "model = model.transform(InferDataTypes())\n",
    "model = model.transform(RemoveStaticGraphInputs())\n",
    "model = model.transform(RemoveUnusedTensors())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.save(\"./multi_model_test/test_temp/model_2D.onnx\")\n",
    "parent_model = model.transform(CreateDataflowPartition())\n",
    "parent_model.save(output_dir + \"/intermediate_models/dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(output_dir + \"/intermediate_models/dataflow_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if all layers are converted to HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving './multi_model_test/test_temp/model_2D.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://0.0.0.0:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f30dcce5b50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(\"./multi_model_test/test_temp/model_2D.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate stitched IP Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataflow accelerator from ./multi_model_test/test_temp/model_2D.onnx\n",
      "Intermediate outputs will be generated in /tmp/finn_dev_wenlong\n",
      "Final outputs will be generated in ./multi_model_test/test_temp/mnist_2D/28\n",
      "Build log is at ./multi_model_test/test_temp/mnist_2D/28/build_dataflow.log\n",
      "Running step: step_create_dataflow_partition [1/8]\n",
      "Running step: step_target_fps_parallelization [2/8]\n",
      "Running step: step_apply_folding_config [3/8]\n",
      "Running step: step_generate_estimate_reports [4/8]\n",
      "Running step: step_hls_codegen [5/8]\n",
      "Running step: step_hls_ipgen [6/8]\n",
      "Running step: step_set_fifo_depths [7/8]\n",
      "Running step: step_create_stitched_ip [8/8]\n",
      "Completed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file = \"./multi_model_test/test_temp/model_2D.onnx\"\n",
    "\n",
    "rtlsim_output_dir = output_dir\n",
    "\n",
    "cfg = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    synth_clk_period_ns = 20.0, #50 MHz         # set target frequency\n",
    "    fpga_part           = \"xc7a50tfgg484-1\",    # set target FPGA board\n",
    "    #fpga_part           = \"xc7a15tfgg484-1\", \n",
    "    \n",
    "    default_mem_mode = build_cfg.ComputeEngineMemMode.CONST,\n",
    "    large_fifo_mem_style = build_cfg.LargeFIFOMemStyle.LUTRAM,   # large FIFO set LUTRAM\n",
    "    #large_fifo_mem_style = build_cfg.LargeFIFOMemStyle.AUTO,\n",
    "    \n",
    "    folding_config_file = \"./multi_model_test/config/Least_pe_malexnet_2D_without_BRAM.json\", # choose a configuration according to requirements\n",
    "    #folding_config_file = \"./multi_model_test/config/Least_pe_malexnet_2D_auto.json\",\n",
    "    \n",
    "    steps = [\"step_create_dataflow_partition\",\n",
    "             \"step_target_fps_parallelization\",\n",
    "             \"step_apply_folding_config\",\n",
    "             \"step_generate_estimate_reports\",\n",
    "             \"step_hls_codegen\",\n",
    "             \"step_hls_ipgen\",\n",
    "             \"step_set_fifo_depths\",\n",
    "            \"step_create_stitched_ip\",],\n",
    "             \n",
    "   \n",
    "\n",
    "    \n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "    ]\n",
    ")\n",
    "\n",
    "build.build_dataflow_cfg(model_file, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
