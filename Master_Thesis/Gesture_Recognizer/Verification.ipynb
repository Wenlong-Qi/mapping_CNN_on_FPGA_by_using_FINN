{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from shutil import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "path1 = \"./Dataset\"\n",
    "classes = ['0', '1', '2', '3', '4', '5','6','7','8','9']\n",
    "n_classes = len(classes)\n",
    "\n",
    "def data_loader(root=path1):\n",
    "        # Define image transforms\n",
    "    train_transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),  # reverse 50% of images\n",
    "            #transforms.RandomRotation(10),      # up to 10 degrees of random rotation\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.5, 0.5, 0.5],     # standard normalization values\n",
    "             #                    [0.5, 0.5, 0.5])\n",
    "            ])   \n",
    "                                                      # In FINN, we prefer to work with bipolar {-1, +1} \n",
    "                                                      # instead of binary {0, 1} values.\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.5, 0.5, 0.5],     # standard normalization values\n",
    "             #                    [0.5, 0.5, 0.5])\n",
    "            ])\n",
    "    train_data = datasets.ImageFolder(os.path.join(root, 'train'), transform=train_transform)\n",
    "    test_data = datasets.ImageFolder(os.path.join(root, 'test'), transform=test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "train_loader, test_loader = data_loader(root=path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[127., 131., 137.,  ..., 146., 140., 136.],\n",
      "          [130., 136., 143.,  ..., 153., 146., 139.],\n",
      "          [135., 141., 149.,  ..., 159., 152., 143.],\n",
      "          ...,\n",
      "          [132., 138., 145.,  ..., 148., 142., 134.],\n",
      "          [127., 132., 138.,  ..., 144., 137., 130.],\n",
      "          [123., 128., 132.,  ..., 138., 133., 128.]],\n",
      "\n",
      "         [[123., 127., 133.,  ..., 145., 139., 135.],\n",
      "          [126., 132., 139.,  ..., 152., 145., 138.],\n",
      "          [131., 137., 145.,  ..., 157., 151., 142.],\n",
      "          ...,\n",
      "          [129., 135., 142.,  ..., 146., 139., 131.],\n",
      "          [124., 129., 135.,  ..., 142., 134., 127.],\n",
      "          [120., 125., 129.,  ..., 136., 131., 125.]],\n",
      "\n",
      "         [[121., 126., 132.,  ..., 140., 135., 132.],\n",
      "          [125., 131., 138.,  ..., 150., 142., 134.],\n",
      "          [130., 136., 144.,  ..., 159., 150., 138.],\n",
      "          ...,\n",
      "          [124., 130., 137.,  ..., 141., 132., 123.],\n",
      "          [119., 124., 130.,  ..., 139., 129., 120.],\n",
      "          [115., 120., 125.,  ..., 135., 129., 119.]]]])\n",
      "tensor([6])\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "cnt = 0\n",
    "\n",
    "for m, (X_train, y_train) in enumerate(train_loader): \n",
    "    #if m == n:\n",
    "        #print(X_train)\n",
    "        #print(y_train)\n",
    "    if cnt>=1: \n",
    "        break    \n",
    "    img = X_train[0]   \n",
    "    img = img.numpy()   \n",
    "    img = np.transpose(img, (1,2,0))\n",
    "    input_tensor = X_train\n",
    "    label = y_train\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(X_train*255)\n",
    "    print(y_train)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    cnt += 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creat test data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "output_txt = input_tensor*255\n",
    "output_txt = output_txt.squeeze()\n",
    "\n",
    "output_txt_R = output_txt[0:1]\n",
    "output_txt_R = (np.array((output_txt_R.squeeze()).view(32*32,-1))).astype(int)\n",
    "np.savetxt('./test_data_32/test_9_R.txt',output_txt_R, fmt=\"%x\")\n",
    "\n",
    "output_txt_G = output_txt[1:2]\n",
    "output_txt_G = (np.array((output_txt_G.squeeze()).view(32*32,-1))).astype(int)\n",
    "np.savetxt('./test_data_32/test_9_G.txt',output_txt_G, fmt=\"%x\")\n",
    "\n",
    "output_txt_B = output_txt[2:3]\n",
    "output_txt_B = (np.array((output_txt_B.squeeze()).view(32*32,-1))).astype(int)\n",
    "np.savetxt('./test_data_32/test_9_B.txt',output_txt_B, fmt=\"%x\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.onnx as bo\n",
    "import onnx\n",
    "import torch\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantMaxPool2d, QuantLinear,QuantDropout,QuantReLU\n",
    "from brevitas.core.quant import QuantType\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet   \n",
    "\n",
    "class Gesture_Recognizer(nn.Module):\n",
    "    def __init__(self,input_size,num_classes,weight_bit_width,act_bit_width):\n",
    "        super(Gesture_Recognizer,self).__init__()\n",
    "        \n",
    "        self.cnn1 = QuantConv2d(in_channels=input_size,out_channels=6,kernel_size=5,\n",
    "                                padding=0,bias=False,weight_bit_width=weight_bit_width)\n",
    "        self.relu1 = QuantReLU(bit_width=act_bit_width)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=None)\n",
    "        \n",
    "        self.cnn2 = QuantConv2d(in_channels=6,out_channels=16,kernel_size=5,\n",
    "                                padding=0,bias=False,weight_bit_width=weight_bit_width)\n",
    "        self.relu2 = QuantReLU(bit_width=act_bit_width)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=None)\n",
    "        \n",
    "        self.cnn3 = QuantConv2d(in_channels=16,out_channels=120,kernel_size=5,\n",
    "                                padding=0,bias=False,weight_bit_width=weight_bit_width)\n",
    "        self.relu3 = QuantReLU(bit_width=act_bit_width)\n",
    "        \n",
    "        \n",
    "        self.drop1 = QuantDropout(p=0.5)\n",
    "        \n",
    "        self.fc1 = QuantLinear(1*1*120,64,bias=False,weight_bit_width=weight_bit_width)\n",
    "        self.relu6 = QuantReLU(bit_width=act_bit_width)\n",
    "        \n",
    "        self.fc2 = QuantLinear(64,num_classes,bias=False,weight_bit_width=weight_bit_width)\n",
    "        self.relu7 = QuantReLU(bit_width=act_bit_width)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.maxpool1(self.relu1(self.cnn1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.cnn2(x)))\n",
    "        x = self.relu3(self.cnn3(x))\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.drop1(x)\n",
    "        x = self.relu6(self.fc1(x))\n",
    "        x = self.relu7(self.fc2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3 # 3 input layers for RGB image\n",
    "num_classes = n_classes\n",
    "weight_bit_width = 4\n",
    "act_bit_width = 4\n",
    "\n",
    "original_model = Gesture_Recognizer(input_size=input_size,num_classes=num_classes,\n",
    "                                    weight_bit_width=weight_bit_width,act_bit_width=act_bit_width)\n",
    "state_dict = torch.load('./test/LeNet_model_parameter_W4A4.pth')\n",
    "original_model.load_state_dict(state_dict)\n",
    "original_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n",
      "input: tensor([[[[0.4980, 0.5137, 0.5373,  ..., 0.5725, 0.5490, 0.5333],\n",
      "          [0.5098, 0.5333, 0.5608,  ..., 0.6000, 0.5725, 0.5451],\n",
      "          [0.5294, 0.5529, 0.5843,  ..., 0.6235, 0.5961, 0.5608],\n",
      "          ...,\n",
      "          [0.5176, 0.5412, 0.5686,  ..., 0.5804, 0.5569, 0.5255],\n",
      "          [0.4980, 0.5176, 0.5412,  ..., 0.5647, 0.5373, 0.5098],\n",
      "          [0.4824, 0.5020, 0.5176,  ..., 0.5412, 0.5216, 0.5020]],\n",
      "\n",
      "         [[0.4824, 0.4980, 0.5216,  ..., 0.5686, 0.5451, 0.5294],\n",
      "          [0.4941, 0.5176, 0.5451,  ..., 0.5961, 0.5686, 0.5412],\n",
      "          [0.5137, 0.5373, 0.5686,  ..., 0.6157, 0.5922, 0.5569],\n",
      "          ...,\n",
      "          [0.5059, 0.5294, 0.5569,  ..., 0.5725, 0.5451, 0.5137],\n",
      "          [0.4863, 0.5059, 0.5294,  ..., 0.5569, 0.5255, 0.4980],\n",
      "          [0.4706, 0.4902, 0.5059,  ..., 0.5333, 0.5137, 0.4902]],\n",
      "\n",
      "         [[0.4745, 0.4941, 0.5176,  ..., 0.5490, 0.5294, 0.5176],\n",
      "          [0.4902, 0.5137, 0.5412,  ..., 0.5882, 0.5569, 0.5255],\n",
      "          [0.5098, 0.5333, 0.5647,  ..., 0.6235, 0.5882, 0.5412],\n",
      "          ...,\n",
      "          [0.4863, 0.5098, 0.5373,  ..., 0.5529, 0.5176, 0.4824],\n",
      "          [0.4667, 0.4863, 0.5098,  ..., 0.5451, 0.5059, 0.4706],\n",
      "          [0.4510, 0.4706, 0.4902,  ..., 0.5294, 0.5059, 0.4667]]]])\n",
      "Label_true: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor.shape)\n",
    "print(\"input:\",input_tensor)\n",
    "print(\"Label_true:\",label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if predict correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_golden: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 4.4067, 0.0000, 6.6100, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<MulBackward0>)\n",
      "Label_pred: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "output_golden = original_model.forward(input_tensor)\n",
    "print(\"output_golden:\",output_golden)\n",
    "\n",
    "output_golden_label = output_golden\n",
    "output_golden_label = torch.max(output_golden_label,1)[1]\n",
    "print(\"Label_pred:\",output_golden_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "from IPython.display import IFrame\n",
    "\n",
    "def showInNetron(model_filename):\n",
    "    netron.start(model_filename, address=(\"0.0.0.0\",8081))\n",
    "    return IFrame(src=\"http://0.0.0.0:8081/\",width=\"100%\",height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import ONNX model from streamline step, and check if predict correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[[[127. 131. 137. ... 146. 140. 136.]\n",
      "   [130. 136. 143. ... 153. 146. 139.]\n",
      "   [135. 141. 149. ... 159. 152. 143.]\n",
      "   ...\n",
      "   [132. 138. 145. ... 148. 142. 134.]\n",
      "   [127. 132. 138. ... 144. 137. 130.]\n",
      "   [123. 128. 132. ... 138. 133. 128.]]\n",
      "\n",
      "  [[123. 127. 133. ... 145. 139. 135.]\n",
      "   [126. 132. 139. ... 152. 145. 138.]\n",
      "   [131. 137. 145. ... 157. 151. 142.]\n",
      "   ...\n",
      "   [129. 135. 142. ... 146. 139. 131.]\n",
      "   [124. 129. 135. ... 142. 134. 127.]\n",
      "   [120. 125. 129. ... 136. 131. 125.]]\n",
      "\n",
      "  [[121. 126. 132. ... 140. 135. 132.]\n",
      "   [125. 131. 138. ... 150. 142. 134.]\n",
      "   [130. 136. 144. ... 159. 150. 138.]\n",
      "   ...\n",
      "   [124. 130. 137. ... 141. 132. 123.]\n",
      "   [119. 124. 130. ... 139. 129. 120.]\n",
      "   [115. 120. 125. ... 135. 129. 119.]]]]\n",
      "Label_true: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "import onnx.numpy_helper as nph\n",
    "\n",
    "input_test1 = input_tensor*255\n",
    "\n",
    "input_test1 = (input_test1.numpy()).astype(np.float32)#.tolist()\n",
    "input_dict = {\"global_in\": input_test1}\n",
    "print(\"input:\",input_test1)\n",
    "print(\"Label_true:\",label)\n",
    "\n",
    "model_for_sim = ModelWrapper(\"./test/output_ipstitch/intermediate_models/2_step_streamline.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Simulation: Label_pred [[6]]\n"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "output_pysim = output_dict[list(output_dict.keys())[0]]\n",
    "print(\"Results for Simulation: Label_pred\",output_pysim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
